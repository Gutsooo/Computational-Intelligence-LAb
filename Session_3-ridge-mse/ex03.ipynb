{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_cost(y, tx, w):\n",
    "    \"\"\"calculate the cost.\n",
    "\n",
    "    you can calculate the cost by mse or mae.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE / MAE\n",
    "    # ***************************************************\n",
    "    \n",
    "    #     raise NotImplementedError\n",
    "    return np.linalg.norm(y-np.dot(tx,w))**2 /(2*y.shape[0])\n",
    "#     return np.sum((y-np.dot(tx,w))**2) / (2*y.size)\n",
    "\n",
    "def compute_cost_MAE(y,tx,w):\n",
    "#     return np.linalg.norm(y-np.dot(tx,w),ord=1)/y.size\n",
    "    return np.sum(abs(y-np.dot(tx,w))) / y.size\n",
    "\n",
    "\n",
    "def grid_search(y, tx, w0, w1):\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\n",
    "    loss = np.zeros((len(w0), len(w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "#     raise NotImplementedError\n",
    "    for i in range(len(w0)):\n",
    "        for j in range(len(w1)) :\n",
    "#             print(np.array([w0[i],w1[j]]).shape) \n",
    "            loss[i][j] = compute_cost(y,tx,np.array([w0[i],w1[j]]))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def compute_gradient_MAE(y,tx,w):\n",
    "    \"\"\"Compute the gradient for MAE function\"\"\"\n",
    "    #     MAE = np.sum(abs(y - np.dot(tx,w)))\n",
    "    # Gradient of MAE is simple. \n",
    "    e = y-np.dot(tx,w)\n",
    "#     grad_w0 = np.sign(e)*-1/y.size\n",
    "#     grad_w1 = np.sign(e)*-1*x1/y.size\n",
    "    return np.dot(tx.T,(-1)* np.sign(e))/y.size\n",
    "    \n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    min_loss = compute_cost(y,tx,w)\n",
    "    counter = 0\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        \n",
    "        # --------------------------- #\n",
    "        # This part is to compute MSE #\n",
    "        # --------------------------- #\n",
    "        grad = compute_gradient(y,tx,w)\n",
    "        loss = compute_cost(y,tx,w)\n",
    "\n",
    "        # --------------------------- #\n",
    "        # This part is to compute MSE #\n",
    "        # --------------------------- #\n",
    "#         grad = compute_gradient_MAE(y,tx,w)\n",
    "#         loss = compute_cost_MAE(y,tx,w)\n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma*grad\n",
    "#         raise NotImplementedError\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        \"\"\"\n",
    "        Interesting here, must we use the isclose?\n",
    "        \"\"\"\n",
    "        if(loss < min_loss): \n",
    "            min_loss = loss\n",
    "            counter = 0\n",
    "        elif (np.isclose(min_loss,loss)): \n",
    "            counter = counter + 1\n",
    "        else: \n",
    "            counter = 0\n",
    "        \n",
    "#         print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "#               bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "#         print(\"counter \",counter)\n",
    "        if (counter > 100):            \n",
    "            print(\"GD stopped for the loss converge to %s\" %loss)\n",
    "            break\n",
    "            \n",
    "\n",
    "    return losses, ws\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1 Least squares and linear basis functions models\n",
    "## 1.1 Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # least squares: TODO\n",
    "    # returns mse, and optimal weights\n",
    "    # ***************************************************\n",
    "    weight = np.linalg.solve(np.dot(tx.T,tx), np.dot(tx.T,y))\n",
    "    return least_square_mse(y,tx, weight),weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_cost_MSE(y, tx, beta):\n",
    "    \"\"\"compute the loss by mse.\"\"\"\n",
    "    e = y - tx.dot(beta)\n",
    "    mse = e.dot(e) / (2 * len(e))\n",
    "    return mse\n",
    "def compute_cost_MAE(y, tx, w):\n",
    "    y = np.array(y)\n",
    "    return np.sum(abs(y - np.dot(tx, w))) / y.shape[0]\n",
    "    \n",
    "def least_square_mse(y, tx, w):\n",
    "    return compute_cost_MSE(y, tx, w)\n",
    "\n",
    "def rmse(y, tx, w):\n",
    "    return np.sqrt(compute_cost_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient and error vector\n",
    "    # ***************************************************\n",
    "    e = y - tx.dot(w)\n",
    "    return -1/len(y) * tx.T.dot(e)\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * gradient\n",
    "        # raise NotImplementedError\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        # print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              # bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Here we will reuse the dataset `height_weight_genders.csv` from previous exercise section to check the correctness of your implementation. Please compare it with your previous result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "def test_your_least_squares():\n",
    "    height, weight, gender = load_data_from_ex02(sub_sample=False, add_outlier=False)\n",
    "    x, mean_x, std_x = standardize(height)\n",
    "    y, tx = build_model_data(x, weight)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # least square or grid search: TODO\n",
    "    w0, w1 = generate_w(num_intervals=1)\n",
    "    initial_w = np.array([w0, w1])\n",
    "    \n",
    "    # LS\n",
    "    ls_w = least_squares(y, tx)\n",
    "    \n",
    "    # GD:\n",
    "#     max_iters = 50\n",
    "#     gamma = 0.7\n",
    "#     gd_weights = gradient_descent(y, tx, initial_w, max_iters, gamma)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('-' * 10)\n",
    "    # print(len(gd_weights))\n",
    "    # print(gd_weights)\n",
    "    print(ls_w)\n",
    "    \n",
    "    # this code should compare the optimal weights obtained \n",
    "    # by least squares vs. grid search\n",
    "    # ***************************************************\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_your_least_squares\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [14], line 9\u001b[0m, in \u001b[0;36mtest_your_least_squares\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m y, tx \u001b[38;5;241m=\u001b[39m build_model_data(x, weight)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# ***************************************************\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# INSERT YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# least square or grid search: TODO\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m w0, w1 \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_w\u001b[49m(num_intervals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m initial_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([w0, w1])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# LS\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_w' is not defined"
     ]
    }
   ],
   "source": [
    "test_your_least_squares()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Least squares with a linear basis function model\n",
    "Start from this section, we will use the dataset `dataEx3.csv`.\n",
    "\n",
    "### Implement polynomial basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "x, y = load_data()\n",
    "print(\"shape of x {}\".format(x.shape))\n",
    "print(\"shape of y {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # polynomial basis function: TODO\n",
    "    # this function should return the matrix formed\n",
    "    # by applying the polynomial basis to the input data\n",
    "    # ***************************************************\n",
    "    x = np.array(x)\n",
    "    res = x\n",
    "    for d in range(2, degree + 1):\n",
    "        res = np.concatenate((res, x ** d), axis=-1)\n",
    "    res = np.reshape(res, (degree, len(x)))\n",
    "    res = np.c_[np.ones((len(res.T), 1)),res.T]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with polynomial regression. Note that we will use your implemented function `compute_mse`. Please copy and paste your implementation from exercise02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import *\n",
    "\n",
    "def polynomial_regression():\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 3, 7, 12]\n",
    "    \n",
    "    # define the structure of the figure\n",
    "    num_row = 2\n",
    "    num_col = 2\n",
    "    f, axs = plt.subplots(num_row, num_col)\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # form the data to do polynomial regression.: TODO\n",
    "        # ***************************************************\n",
    "        x_degree = build_poly(x,degree)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # least square and calculate RMSE: TODO\n",
    "        # ***************************************************\n",
    "        lsq_degree, weight = least_squares(y,x_degree)\n",
    "#         print(weight)\n",
    "        rmse = np.sqrt(2*lsq_degree)\n",
    "        print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "              i=ind + 1, d=degree, loss=rmse))\n",
    "        # plot fit\n",
    "        plot_fitted_curve(\n",
    "            y, x, weight, degree, axs[ind // num_col][ind % num_col])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"visualize_polynomial_regression\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Evaluating model predication performance\n",
    "\n",
    "\n",
    "Let us show the train and test splits for various polynomial degrees. First of all, please fill in the function `split_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # **************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data based on the given ratio: TODO\n",
    "    # ***************************************************\n",
    "    pair = np.c_[x,y]\n",
    "    np.random.shuffle(pair)\n",
    "    index = np.round(x.size * ratio,0).astype('int16')\n",
    "    p1, p2 = np.split(pair,[index])\n",
    "    x1,y1 = zip(*p1)\n",
    "    x2,y2 = zip(*p2)\n",
    "    return x1,y1,x2,y2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, test your `split_data` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data, and return train and test data: TODO\n",
    "    # ***************************************************\n",
    "    trainX,trainY,testX,testY = split_data(x,y,ratio,seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    # ***************************************************\n",
    "    #     print(len(trainX))\n",
    "    # trainX = np.c_[np.ones((len(trainX),1)), build_poly(trainX,degree)]\n",
    "    # testX = np.c_[np.ones((len(testX),1)), build_poly(testX,degree)]\n",
    "    trainX = build_poly(trainX, degree)\n",
    "    testX = build_poly(testX, degree)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calcualte weight through least square.: TODO\n",
    "    # ***************************************************\n",
    "    mse, weight = least_squares(trainY,trainX)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate RMSE for train and test data,\n",
    "    # and store them in rmse_tr and rmse_te respectively: TODO\n",
    "    # ***************************************************\n",
    "    mse_test = np.sum((testY-np.dot(testX,weight))**2)/len(testY)\n",
    "    rmse_tr = np.sqrt(2*mse)\n",
    "    rmse_te = np.sqrt(2*mse_test)\n",
    "    print(\"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, tr=rmse_tr, te=rmse_te))\n",
    "    return mse, weight, trainX, trainY, testX, testY \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 6\n",
    "degrees = [1, 3, 7, 12]\n",
    "split_ratios = [0.9, 0.5, 0.1]\n",
    "\n",
    "# num_row = len(degrees)\n",
    "# num_col = len(split_ratios)\n",
    "# f, axs = plt.subplots(num_row, num_col)\n",
    "  \n",
    "for split_ratio in split_ratios:\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        mse, weight, trainX, trainY, testX, testY = train_test_split_demo(x, y, degree, split_ratio, seed)\n",
    "        # plot_fitted_curve(trainY, trainX, weight, degree, axs[ind // num_col][ind % num_col])\n",
    "        # plot_fitted_curve(testY, testX, weight, degree, axs[ind // num_col + 1][ind % num_col + 1])\n",
    "        \n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(\"visualize_polynomial_regression\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Ridge Regression\n",
    "Please fill in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    lambda_ *= 2 * tx.shape[1]\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    G = np.eye(tx.shape[1])\n",
    "    hes = np.dot(tx.T,tx) + lambda_ * G\n",
    "    weight = np.linalg.solve(hes,np.dot(tx.T,y))\n",
    "    mse = compute_cost_MSE(y, tx, weight)\n",
    "    \n",
    "    return mse, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)    \n",
    "    \n",
    "    # split data\n",
    "    trainX,trainY,testX,testY = split_data(x,y,ratio,seed)\n",
    "    print(trainX, trainY)\n",
    "    trainX = build_poly(trainX,degree)\n",
    "    testX = build_poly(testX,degree)\n",
    "    \n",
    "    _rmse_te = []\n",
    "    _rmse_tr = []\n",
    "    \n",
    "    for ind, lamb in enumerate(lambdas):\n",
    "        mse, weight = ridge_regression(trainY,trainX,lamb)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # calculate RMSE for train and test data,\n",
    "        # and store them in rmse_tr and rmse_te respectively: TODO\n",
    "        # ***************************************************\n",
    "        mse_test = compute_cost_MSE(testY, testX, weight)\n",
    "        \n",
    "        # calculate rmse\n",
    "        rmse_tr = np.sqrt(2*mse)\n",
    "        rmse_te = np.sqrt(2*mse_test)\n",
    "        \n",
    "        # append to log\n",
    "        _rmse_te.append(rmse_te)\n",
    "        _rmse_tr.append(rmse_tr)\n",
    "        print(\"lambda={l}, proportion={p}, degree={d}, weight={w}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "              l=ind, p=ratio, d=degree, w=len(weight), tr=rmse_tr, te=rmse_te))  \n",
    "    print(_rmse_te, _rmse_tr)\n",
    "    plot_train_test(_rmse_tr, _rmse_te, lambdas, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 56\n",
    "degree = 7\n",
    "split_ratio = 0.5\n",
    "ridge_regression_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
